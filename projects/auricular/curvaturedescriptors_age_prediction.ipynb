{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auricular shape analysis - age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "\n",
    "from projects.auricular.curvaturedescriptors import CurvatureDescriptorsParams\n",
    "from projects.auricular.curvaturedescriptors import CurvatureDescriptors\n",
    "from projects.auricular.curvaturedescriptors import HistogramDescriptors\n",
    "from projects.auricular.analyze import ModelAnalysis\n",
    "from projects.auricular.common import getSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = os.path.expanduser(\"~/data/aurikularni_plocha_ply5/\")\n",
    "sample = list(getSample(input_data))\n",
    "params = CurvatureDescriptorsParams(\n",
    "    input_data=input_data,\n",
    "    sampling_method='regular',\n",
    "    dist=1.0,\n",
    "    sampling_rate=0.5,\n",
    "    sample_count=5000,\n",
    "    output='../../../output_812')\n",
    "cd = CurvatureDescriptors(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cd.getData()\n",
    "hist_descriptors = {\n",
    "    0.5: HistogramDescriptors(data, 0.5),\n",
    "    1.0: HistogramDescriptors(data, 1.0),\n",
    "    2.0: HistogramDescriptors(data, 2.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age x mean dne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from projects.auricular.analyze import evaluateAllModels, ModelAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: ['logAge'] ~ ['dne']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvalue</th>\n",
       "      <th>rmse</th>\n",
       "      <th>bias</th>\n",
       "      <th>inaccuracy</th>\n",
       "      <th>indep</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.147024</td>\n",
       "      <td>7.606958e-01</td>\n",
       "      <td>13.326758</td>\n",
       "      <td>[dne]</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18.948460</td>\n",
       "      <td>-5.600337e-15</td>\n",
       "      <td>15.990033</td>\n",
       "      <td>[mean]</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.612774</td>\n",
       "      <td>9.499993e-01</td>\n",
       "      <td>21.811334</td>\n",
       "      <td>[random]</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pvalue       rmse          bias  inaccuracy     indep subset\n",
       "0       0  16.147024  7.606958e-01   13.326758     [dne]    all\n",
       "1       0  18.948460 -5.600337e-15   15.990033    [mean]    all\n",
       "2       0  26.612774  9.499993e-01   21.811334  [random]    all"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=[[\n",
    "                    float(data1['age']),\n",
    "                    np.log(float(data1['age'])),\n",
    "                    float(data1['dist'][1.0]['sampled_dne'])] for data1 in data],\n",
    "                  columns = ['age', 'logAge', 'dne'])\n",
    "\n",
    "r = evaluateAllModels(df, indeps=[['dne']], dep=['logAge'], model=SVR())\n",
    "pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age x vector dne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist_descriptors[0.5].getSampleHistogramData(3)[0]).plot.bar()\n",
    "pd.DataFrame(hist_descriptors[0.5].getSampleHistogramData(3)[249]).plot.bar()\n",
    "pd.DataFrame(hist_descriptors[0.5].getSampleHistogramData(3)[498]).plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma=ModelAnalysis(data, hist_descriptors, 'dist_curv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.twoParamPlot(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.modelForBins(3, None, 1.0, model=SVR(), normalize_dist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.modelForBins(3, [[0, 2]], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.modelForBins(3, [[0, 1]], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.plotRmsePerBins(list(ma.binsRmse(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: ['logAge'] ~ [0, 1]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3hUVfrA8e+bTkIPnSSELgjSQseOiNi7qKsuKvZdXV3rWtZttt217qo/RWwgqMiisgKuBQUEQgkECAQhkgKEFgiE1Hl/f8xFQkxIMpPkJpP38zzzzJ1bzryZ3HnnzDlnzhVVxRhjTOAKcjsAY4wxtcsSvTHGBDhL9MYYE+As0RtjTICzRG+MMQEuxO0AytOmTRuNj493OwxjjGkwVqxYsVtV25a3rV4m+vj4eBITE90OwxhjGgwR+amibdZ0Y4wxAa7SRC8iU0QkW0SSS60bICJLRGStiHwqIs0rODbN2We1iFgV3RhjXFCVGv1UYHyZdW8AD6pqf+AT4PfHOf50VR2oqgm+hWiMMcYflbbRq+pCEYkvs7oXsNBZXgDMAx6t0ciMMaYKioqKyMjIID8/3+1Q6kRERAQxMTGEhoZW+RhfO2PXARcCs4HLgdgK9lNgvogo8Jqqvl5RgSIyGZgMEBcX52NYxpjGJiMjg2bNmhEfH4+IuB1OrVJV9uzZQ0ZGBl27dq3ycb52xk4CbheRFUAzoLCC/cao6mDgHOAOETmlogJV9XVVTVDVhLZtyx0hZIwxv5Cfn090dHTAJ3kAESE6Orra3158SvSqmqKq41R1CDAd+LGC/TKd+2y8bfnDfHk+Y4w5nsaQ5I/w5W/1KdGLSDvnPgj4A/BqOftEiUizI8vAOCC57H7GBJKFm3aRmLbX7TCMOUZVhldOB5YAvUUkQ0RuBCaKyCYgBcgC3nL27SQic51D2wPfi0gSsAz4XFW/qI0/wpj6IDlzP5OmLueK15bwxndbsGs9mPqiKqNuJlaw6YVy9s0CJjjLW4ABfkVnTAORX1TC72aupnVUGIPiWvLnzzewLusAf7ukPxGhwW6HZ+qQqqKqBAXVn9+j1p9IjGnA/rFgE5t2HuTpy07i1WuHcO9ZvfhkVSZXvLaE7fsPux2eqWVpaWn07t2b6667jqZNm9K9e3duuOEGevXqxTXXXMOXX37J6NGj6dmzJ8uWLQPg22+/ZeDAgQwcOJBBgwaRm5sLwLPPPsvQoUM56aSTePzxx2skvno5140xDckPW/bwf99t4erhcZzeux0Ad53ZkxM6NueeGas5/6VFvHrtYBLiW7scaeD746frWJ91oEbL7NupOY+ff2Kl+6WmpvL222/z5JNP0qNHD+69916mTJnC0KFDmTZtGt9//z1z5szhr3/9K7Nnz+a5557jlVdeYfTo0Rw8eJCIiAjmz59Pamoqy5YtQ1W54IILWLhwIaecUuGAxSqxGr0xfsjNL+K+D5OIax3JIxP6HLPtrL7t+eT2UTQND2bi//3A9GXbXIrS1IUuXbowYsQIALp27Ur//v0JCgrixBNP5Mwzz0RE6N+/P2lpaQCMHj2a3/3ud7z44ovk5OQQEhLC/PnzmT9/PoMGDWLw4MGkpKSQmprqd2xWozfGD3/+bANZOYf58NaRRIX/8u3Us30z/nPHGO76YBUPzVrLhu0HePS8voQGWx2rNlSl5l1boqKifl4ODw//eTkoKOjnx0FBQRQXFwPw4IMPcu655zJ37lxGjx7NvHnzUFUeeughbrnllhqNzc42Y3y0YP1OZiSmc+up3RnSpeJmmRaRobx1w1BuOaUb7yz5iWvfWMqegwV1GKmpj3788Uf69+/PAw88wNChQ0lJSeHss89mypQpHDx4EIDMzEyys7P9fi6r0Rvjgz0HC3ho1hr6dGzO3WN7Vbp/cJDw0IQ+9O3UnPs/WsMFLy/i9euGcGKnFnUQramPnn/+eb7++uufm3fOOeccwsPD2bBhAyNHjgSgadOmvPfee7Rr186v55L6ONY3ISFB7cIjpr5SVW59bwVfp+xizl2jOaFDubN0V2htxn4mv5vIvrxCnr1sAOcP6FRLkTYOGzZsoE+fPpXvGEDK+5tFZEVFswRb040x1TRrZSbz1u3k3nG9qp3kAfrHtGDOnWPo37kFd01fxTNfpFDiqX8VLhM4LNEbVyzbupd/LNhEQXGJ26FUS2bOYZ6Ys45h8a256eRuPpfTtlk47980gquHx/Gvb37kpreXcyC/qAYjNeYoS/Smzm3amcukqct58X+p/Pqt5eQ2kATn8Sj3zUzCo8pzlw8gOMi/ibTCQoL468X9+fNF/fgudTcXvbKIH3cdrKFoG5f62ARdW3z5Wy3Rmzq152ABN769nIjQYP5wbh+Wbd3LFa/9QPaB+n/RiKmL01iyZQ+PnteXuOjIGiv32hFdmHbzCPbnFXHRy4v4OsX/URaNSUREBHv27GkUyf7IfPQRERHVOs46Y02dKSgu4do3lpKUsZ8Zk0cwKK4VCzft4tb3VtAqMoy3Jw2jR7umbodZrs3ZuZz74veM6dGGN65PqJVpcTNzDnPLu4msyzrA78/uzW2ndm9U0+/6yq4w5XW8zlhL9KZOqCq//2gNH63I4MWJg7ig1EiTtRn7+fXUZRR7lDevH8qQLq1cjPSXiko8XPKvxWTsy2PePafQrln1alPVcbiwhAc+XsOcpCxG94jmwfF96B9jQzBN5WzUjXHdawu38NGKDH57Zs9jkjx4R6HMum00LZuEcvX//cCC9TtdirJ8L321mbWZ+/nrxf1rNckDNAkL5oWrBvKnC09kfdYBzn/5e+6YtpKtuw/V6vOawGaJ3tS6eet28PQXKZx3UkfuHtuz3H3ioiP5+LZRnNChGbe8m8i0pfVjXpjV6Tm88vVmLhnUmXP6d6yT5xQRfjUynm/vP527zujBVxuyOesf3/LIJ2sbRF9GWarK6vQcHpq1htOe/ZqX/pdKYbHH7bAaFWu6MbVqXdZ+Ln91CT3bNWXGLSMrnZs9r7CYO95fydcbd/GbM3tyz9ierrVTHy4s4dyXviO/sIT/3n0KLZqEVn5QLcjOzeflrzYzbek2QoODmDQmnltO7U7zCHfiqaq9hwr5ZFUmM5ens3FnLk1Cg+nbqTkrftpHj3ZN+ctF/RjeLdrtMAOGtdEbV2Tn5nPRy4tQ4D93jKZd86o1exSXeHj4k7XMTMzgyoRY/nJxP0JcmATsiTnrmLo4jfdvGs7oHm3q/PnLStt9iL8v2MSnSVm0jAzljtN68KuRXerVhU08HuX7zbuZsTydBet3UljiYUBsS65MiOX8AR1pFhHK1xuzeXR2Mhn7DnNFQgwPndOHVlFhbofe4FmiN3Uuv6iEK1//gU07cvnw1pH061y9DkVV5Z8LNvHiV5s544R2vHz1ICLD6m5qpu9Td3Ptm0v59eh4V2dELE9y5n6e/iKF71J306lFBHef1YtLB8f4Pa7fHxn78vgwMYOPVmSQmXOYlpGhXDyoM1cOjS3318OHC0t44X+pvPHdFpo3CeWRCX24ZHBnG2XkB0v0pk6pKr/5YDWfJmXx6rVDGN+vg89lvb/0Jx6dnUz/mJZMuT6B6KbhlR/kp/2Hixj//EIiw4L5/Dcn16sac2mLN+/m6S9SSMrYT892Tfn92b05q2/7OkuWBcUl3hk8l6fz/ebdAIzp0YYrh8ZyVt/2hIdU/rql7DjAw7PWsnJbDqO6R/Pni/rRrW39HGJb31miN3XqhS9T+eeXm7h/fG9uP62H3+XNW7eD30xfRaeWTXhn0jBiW9fcj5XKc8+M1cxJymLWbaMYENuyVp/LX6rKF8k7eHbeRrbsPsSQLq14YPwJDOtae1ezStlxgBnL05m9KpN9eUV0ahHB5QmxXJ4QQ0yr6v9vPB5l+vJtPPXfFAqKPNx+enduO617lT4ozFGW6E2d+WxNFndOW8Ulgzvz98sH1FjtMjFtLze+nUhocBBTfz202k1BVTV37XZuf38ld4/tWaXph+uL4hIPMxMzeP7LTWTnFnDGCe24f3xvnyZdK09ufhGfJm1nRmI6Sek5hAYL4/p24IqhsYzp0aZGmo2yc/P582cbmJOURbe2Ufzlov6M7G6dtVVlid7UiaT0HK54bQn9O7fg/ZuH13iNbHN2LtdPWU5OXiGv/moIJ/dsW6PlZx/I5+znFxLb2jvUsyFeBepwYQlvLd7Kv7/5kYMFxVw8sDP9OregoNhDflEJ+cUlFBR5KHDu84tLyHcel77PLyr5+ZiCYs/PwyF7tW/KlUPjuHhQZ1rXUgfqt5t28ejsZLbtzePSwTE8cm6fWnuu+qKoxMPGHbnsPJDPmX3a+1SGX4leRKYA5wHZqtrPWTcAeBVoCqQB16jqL67IKyLjgReAYOANVX2qKgFbom94tu8/zIUvLyIsJIj/3DG61trSdx7I5/opy9icfZBnLz+JiwfF1Ei5qsqNbyeyaPNuPv/NyfV2Koaqyskr5N/f/MjUxWkUlBqzHhYcRHhoEBGhwYSHHHsfERpEeMgv78NDg4gMDeGUXm0YGNuyTvoA8otKeOmrVF77dgvNIkJ4aEIfLh8SExCdtapKxr7DrE7PYXV6DknpOSRn7Se/yEPziBBWPzaOIB++Ifmb6E8BDgLvlEr0y4H7VPVbEZkEdFXVR8scFwxsAs4CMoDlwERVXV9ZwJboG5a8wmIuf3UJP+3JY9bto+jVvlmtPt+B/CJueWcFS7bs4cFzTuCWU7pVmACKSzwcKijhYGExeQXFHCwoJq+whIMFxRwqKOZQYQmHCorZuusQMxLTefz8vvx6dNdajb8u5RUWU1js8SbskCCfEoibNu3M5eFZa0n8aR/Du7bmLxf3b3AfwvsPF5HkJPTV6TkkZeSw+2AhAOEhQfTr3IKBsS1/vsW0auLTB5rfTTciEg98VirR7wdaqqqKSCwwT1X7ljlmJPCEqp7tPH4IQFX/VtnzWaJvODwe5bb3V7Bg/U7evGEop/f275JnVVVQXMK9M5P4bM12RnaLJiRYyHOSdulkXtVfYIrAhH4deWnioAaXDAOdx6PMSEznb3M3cLiohNtO7c7tp/eocDSUx6MUlni8TVQlR5qqnOaqYs/RpqtiDyUepUlYMFFhIUSGBRMVHkKUc98kNLja50JhsYcN2w+QlJHD6m05rM7IYcsu7/QVItC9bdNjknrvDs1qrInweIne14HJ64ALgdnA5UBsOft0BtJLPc4Ahh8nyMnAZIC4uDgfwzJ17bn5G5m3biePnde3zpI8QHhIMC9eNYj46CjmrdtBZHgITcODaR0VSdPwEKLCvW/eqPCQY968v1gfHkzT8BAiQqr/pjZ1IyhImDgsjrF92vOXz9fz4lebmZGYTqvIMAqLSyVxJ6EXltTc9AqRYcFEhnnPk8gw73kU6ZxPpdfnF5WQlJHDuqwDP1cu2jYLZ2BsSy4dHMPA2Jb0j2nh2q+Zfa3RnwC8CEQDc4DfqGp0mWMuA8ar6k3O418Bw1X1zsqez2r0DcPHKzK498MkJg6L468X9wuI9lNT/32Xuot3l/wEQLjTx+C9efsTjvRDHGmuCg8JIjw0uNT6o9uCg4TDRSXkFZRwqLCYvMJiDhWUHHtfWEKe08xX0fZgEfrHHNsE07FFRJ2+J2q8Rq+qKcA4p/BewLnl7JbJsTX9GGedCQDL0/by0Ky1jOoezZMXnmhJ3tSZk3u2rfERV/5S1Xr9HvCpcUhE2jn3QcAf8I7AKWs50FNEuopIGHAV3tq/aeDS9+Zxy7sr6NyqCf+6ZnCDHIZoTE2qz0keqpDoRWQ6sAToLSIZInIjMFFENgEpQBbwlrNvJxGZC6CqxcCdwDxgAzBTVdfVzp9h6kpufhE3vr2c4hIPb16fQMvIwB7fbEwgqLTpRlUnVrDphXL2zQImlHo8F5jrc3Sm3nliznp+3HWIdyYNszlJjGkg7Du3qbJvNmbz8coMbju1e72YttcYUzWW6E2VHCwo5pFPkunRril3nen/RGXGmLpTdxN8mwbt6f+mkLX/MB/dOspmFTSmgbEavanU0i17ePeHn5g0uitDurRyOxxjTDVZojfHdbiwhAc+XkNc60juG9fb7XCMMT6wphtzXP/8chNpe/KYdvNwmoRZk40xDZHV6E2FktJzeOO7LUwcFseo7jbKxpiGyhK9KVdhsYf7P1pDu2YRPDThBLfDMcb4wZpuTLle+XozG3fmMuWGBNdm3DPG1Ayr0Ztf2LD9AK98vZmLB3XmjBN8u6yZMab+sERvjlFc4m2yaRkZymPn9a38AGNMvWdNN+YYb3y/lbWZ+3nl6sG0CvALMhvTWFiN3vxsy66D/HPBJs4+sT0T+ndwOxxjTA2xRG8A73U2H/h4DRGhwfzpQrtalDGBxBK9AeDdH35iedo+Hj2vL+2aR7gdjjGmBlmiN6TvzePpL1I4tVdbLh3c2e1wjDE1zBJ9I6eqPPzJWgT46yX9rcnGmABkib6R+3BFBt+l7ubBc06gc8smbodjjKkFlugbsZ0H8vnzZ+sZ1rU11wzv4nY4xphaYom+kVJV/jA7mYJiD09fehJBQdZkY0ygskTfSH22ZjsL1u/k3nG96Nomyu1wjDG1qNJELyJTRCRbRJJLrRsoIj+IyGoRSRSRYRUcW+Lss1pE5tRk4MZ3ew8V8sScdQyIacGk0V3dDscYU8uqUqOfCowvs+4Z4I+qOhB4zHlcnsOqOtC5XeB7mKYm/fHTdRzIL+KZywYQEmxf6owJdJW+y1V1IbC37GqgubPcAsiq4bhMLfly/U7+szqLO07vQe8OzdwOxxhTB3yd1OxuYJ6IPIf3w2JUBftFiEgiUAw8paqzKypQRCYDkwHi4uJ8DMscz/7DRTwyey0ndGjG7af1cDscY0wd8fV7+23APaoaC9wDvFnBfl1UNQG4GnheRLpXVKCqvq6qCaqa0LZtWx/DMsfzt7kb2JVbwDOXnURYiDXZGNNY+Ppuvx6Y5Sx/CJTbGauqmc79FuAbYJCPz2f8tGjzbj5Yns7Np3TjpJiWbodjjKlDvib6LOBUZ/kMILXsDiLSSkTCneU2wGhgvY/PZ/xwqKCYB2etoWubKO4Z28vtcIwxdazSNnoRmQ6cBrQRkQzgceBm4AURCQHycdrWRSQBuFVVbwL6AK+JiAfvB8pTqmqJ3gXPzttIxr7DzLxlJBGhwW6HY4ypY5UmelWdWMGmIeXsmwjc5CwvBvr7FZ3xW2LaXt5eksZ1I7owNL612+EYY1xgPXIBLL+ohPs/XkOnFk24f/wJbodjjHGJXTM2gL3wv1S27DrEO5OGERVu/2pjGiur0Qeo5Mz9vL5wC1ckxHBKLxuuakxjZok+ABUWe7jvwySio8J45Ny+bodjjHGZfZ8PQK9++yMpO3J5/VdDaNEk1O1wjDEusxp9gNm0M5eXvkrl/AGdGHdiB7fDMcbUA5boA0iJR7n/ozU0iwjlifOtycYY42WJPoC8tWgrq9NzePz8vkQ3DXc7HGNMPWGJPkCk7T7Es/M2MrZPey4Y0MntcIwx9Ygl+gDg8SgPfLyGsJAg/nJxP0Ts+q/GmKMs0QeAacu2sXTrXv5wbh/aN49wOxxjTD1jib6By8w5zN/mbmBMjzZckRDrdjjGmHrIEn0Dpqo8PGstCvztkv7WZGOMKZcl+gZs1spMvt20i/vP7k1s60i3wzHG1FOW6Buo7Nx8nvxsPQldWnHdyHi3wzHG1GOW6Buox2av43BRCU9fdhJBQdZkY4ypmCX6Bmju2u18sW4H94ztRfe2Td0OxxhTz1mib2D2HSrksf8k079zC24+uavb4RhjGgCbvbKBefKz9eTkFfHOpOGEBNvntDGmcpYpGpCvUnbyyapMbj+9B307NXc7HGNMA2GJvoE4kF/Ew7OS6dW+KXee3sPtcIwxDUiVEr2ITBGRbBFJLrVuoIj8ICKrRSRRRIZVcOz1IpLq3K6vqcAbm7/NTSE7N59nLhtAWIh9Phtjqq6qGWMqML7MumeAP6rqQOAx5/ExRKQ18DgwHBgGPC4irXyOtpFavHk305dt46aTuzEwtqXb4RhjGpgqJXpVXQjsLbsaONJQ3ALIKufQs4EFqrpXVfcBC/jlB4Y5jrzCYh6ctZb46EjuGdvL7XCMMQ2QP6Nu7gbmichzeD8wRpWzT2cgvdTjDGfdL4jIZGAyQFxcnB9hBZYn5qwjfV8eH9w8giZhwW6HY4xpgPxp7L0NuEdVY4F7gDf9CURVX1fVBFVNaNu2rT9FBYz/rM5kZmIGd5zWg+Hdot0OxxjTQPmT6K8HZjnLH+Jtgy8rEyg9d26Ms85UIm33IR6etZaELq24e2xPt8MxxjRg/iT6LOBUZ/kMILWcfeYB40SkldMJO85ZZ46joLiEO6evJCQ4iBcmDrIfRhlj/FKlNnoRmQ6cBrQRkQy8I2luBl4QkRAgH6d9XUQSgFtV9SZV3SsifwKWO0U9qaplO3VNGc98sZHkzAO89qshdG7ZxO1wjDENXJUSvapOrGDTkHL2TQRuKvV4CjDFp+gaof9t2Mmb32/l+pFdOPvEDm6HY4wJANYmUI9s33+Y+z5Mom/H5jw0oY/b4RhjAoQl+nqixKP89oPVFBR7eOnqQUSE2lBKY0zNsNkr64kX/5fKsq17+fvlA2yOeWNMjbIafT2w5Mc9vPRVKpcM7sylQ2LcDscYE2As0bts76FC7p6xivjoKP50YT+3wzHGBCBrunGRqnLfh0nsO1TElBuGEhVu/w5jTM2zGr2L3vx+K1+lZPPIuX04sVMLt8MxxgQoS/QuWZORw9NfpDCub3uuG9nF7XCMMQHMEr0LcvOLuHPaKto2DeeZy05CRNwOyRgTwKxRuI6pKg9/kkxmzmFmTB5By8gwt0MyxgQ4q9HXsZmJ6XyalMU9Y3uSEN/a7XCMMY2AJfo6lLozl8fnrGN0j2huO80u8G2MqRuW6OtIflEJd05bRVRYCP+8YiDBQdYub4ypG9ZGX0ee/Gw9G3fm8vakYbRrHuF2OMaYRsRq9HXg8zXbmbZ0G7ec2o1Te9llEo0xdcsSfS1L35vHg7PWMDC2JfeN6+12OMaYRsgSfS0qKvFw1/RVALw0cRChdklAY4wLrI2+Fj03fyOr03N45erBxLaOdDscY0wjZVXMWrJh+wFe+3YLVw+P49yTOrodjjGmEbNEX0veWrSVJqHBPHD2CW6HYoxp5CzR14I9BwuYvTqLSwZ3pkVkqNvhGGMauUrb6EVkCnAekK2q/Zx1M4AjQ0haAjmqOrCcY9OAXKAEKFbVhBqKu177YHk6hcUefj063u1QjDGmSp2xU4GXgXeOrFDVK48si8jfgf3HOf50Vd3ta4ANTVGJh3eX/MTJPdvQo10zt8MxxpjKm25UdSGwt7xt4p1f9wpgeg3H1WD9N3kHOw7kW23eGFNv+NtGfzKwU1VTK9iuwHwRWSEik/18rgZh6qKtxEdHclqvdm6HYowxgP+JfiLHr82PUdXBwDnAHSJySkU7ishkEUkUkcRdu3b5GZY7ktJzWLkth+tHxRNkk5YZY+oJnxO9iIQAlwAzKtpHVTOd+2zgE2DYcfZ9XVUTVDWhbduGOR/M1MVpNA0P4bIhMW6HYowxP/OnRj8WSFHVjPI2ikiUiDQ7sgyMA5L9eL56LftAPp+tyeKyITE0i7AhlcaY+qPSRC8i04ElQG8RyRCRG51NV1Gm2UZEOonIXOdhe+B7EUkClgGfq+oXNRd6/fLe0m0Ue5QbRsW7HYoxxhyj0uGVqjqxgvU3lLMuC5jgLG8BBvgZX4NQUFzCtKU/cUbvdsS3iXI7HGOMOYb9MrYGfJa0nd0HC7nBhlQaY+ohS/R+UlWmLk6jR7umjOnRxu1wjDHmFyzR+2nFT/tYm7mfG0bF4/39mDHG1C+W6P301qI0mkeEcMngzm6HYowx5bJE74esnMN8sW4HVw2LIzLMruFijKmfLNH74d0ffkJVuW5kF7dDMcaYClmi91F+UQnTl21jXN8OxLSyywQaY+ovS/Q+mr0qk5y8IhtSaYyp9yzR++DIkMo+HZszvGtrt8MxxpjjskTvgyVb9pCyI5df25BKY0wDYIneB28tSqN1VBgXDOzkdijGGFMpS/TVlL43jy837OTqYXFEhAa7HY4xxlTKEn01vb04jWARrh1hQyqNMQ2DJfpqOFRQzIzEdM7p35EOLSLcDscYY6rEEn01zFqZQW5+sc05b4xpUCzRV5HHo7y1OI0BMS0YHNfS7XCMMabKLNFX0Xebd7Nl1yFuGG1DKo0xDYsl+ip6a9FW2jYL59z+NqTSGNOwWKKvgi27DvLNxl1cO7wLYSH2khljGhbLWlXw9uI0woKDuHp4nNuhGGNMtVmir8SB/CI+WpHBeQM60rZZuNvhGGNMtVmir8SHiRkcKizh16O6uh2KMcb4pNJELyJTRCRbRJJLrZshIqudW5qIrK7g2PEislFENovIgzUZeF0o8ShvL04joUsr+se0cDscY4zxSVVq9FOB8aVXqOqVqjpQVQcCHwOzyh4kIsHAK8A5QF9gooj09TviOvR1Sjbb9ubZnPPGmAat0kSvqguBveVtE++A8iuA6eVsHgZsVtUtqloIfABc6Eesde6txVvp2CKCs0/s4HYoxhjjM3/b6E8GdqpqajnbOgPppR5nOOvKJSKTRSRRRBJ37drlZ1j+27gjl0Wb9/CrkV0IDbauDGNMw+VvBptI+bX5alPV11U1QVUT2rZtWxNF+mXq4jTCQ4KYONSGVBpjGrYQXw8UkRDgEmBIBbtkArGlHsc46+q9nLxCPlmVwcWDOtMqKsztcIwxxi/+1OjHAimqmlHB9uVATxHpKiJhwFXAHD+er858sDyd/CKPdcIaYwJCVYZXTgeWAL1FJENEbnQ2XUWZZhsR6SQicwFUtRi4E5gHbABmquq6mgy+NhSXeHh3yU+M7BbNCR2aux2OMcb4rdKmG1WdWMH6G8pZlwVMKPV4LjDXj/jq3Dcbd5GZc5hHz+vjdijGGFMjbDhJGdOXbaNts3DO7NPe7VCMMaZGWKIvJSvnMF9vzOaKhBgbUmmMCRiWzUqZmZiOAlfZkEpjTNCJ0LEAAAyUSURBVACxRO8o8Sgzlqdzcs+2xLaOdDscY4ypMZboHd9uymb7/nyuHhZb+c7GGNOAWKJ3TFu6jTZNrRPWGBN4LNED2/cf5qsU64Q1xgQmy2rAzOUZeNQ6YY0xganRJ3pvJ+w2Tu7Zhrho64Q1xgSeRp/oF27aRdb+fCYOs9q8MSYwNfpEP23ZNto0DWOsdcIaYwJUo070O/bn81VKNpcnxBIW0qhfCmNMAGvU2e3DxHRKPMpVQ23svDEmcDXaRF/iUT5Yns6YHm3oEh3ldjjGGFNrGm2i/y7VOx2xdcIaYwJdo03005dtIzoqjLP6WiesMSawNcpEv/NAPl9uyOayhBjrhDXGBLxGmeWOdsJas40xJvA1ukTv8SjTl6Uzqns0XdtYJ6wxJvA1ukT/3ebdZOYc5urhVps3xjQOjS7RT1/q7YQd17eD26EYY0ydqDTRi8gUEckWkeQy6+8SkRQRWSciz1RwbJqIrBWR1SKSWFNB+yr7QD5fbtjJZUOsE9YY03iEVGGfqcDLwDtHVojI6cCFwABVLRCRdsc5/nRV3e1XlDXkwxUZFHuUK+2XsMaYRqTSaq2qLgT2lll9G/CUqhY4+2TXQmw1yuNRPli+jZHdounWtqnb4RhjTJ3xtf2iF3CyiCwVkW9FZGgF+ykwX0RWiMjk4xUoIpNFJFFEEnft2uVjWBVb9ONu0vceZqJ1whpjGpmqNN1UdFxrYAQwFJgpIt1UVcvsN0ZVM52mnQUikuJ8Q/gFVX0deB0gISGhbDl+m75sG60iQzn7RPslrDGmcfG1Rp8BzFKvZYAHaFN2J1XNdO6zgU+AYb4G6o/s3Hzmr/N2woaHBLsRgjHGuMbXRD8bOB1ARHoBYcAxHa4iEiUizY4sA+OAZFzwkdMJaxOYGWMao6oMr5wOLAF6i0iGiNwITAG6OUMuPwCuV1UVkU4iMtc5tD3wvYgkAcuAz1X1i9r5Myrm8SgfLEtnRLfW1glrjGmUKm2jV9WJFWy6tpx9s4AJzvIWYIBf0dWAxT/uYdvePO4d18vtUIwxxhUB/6uho52w9ktYY0zjFNCJflduAfPW7eDSwTFEhFonrDGmcQroRP/xSm8n7FXWCWuMacQCNtF7O2G3Maxra3q0s05YY0zjFbCJ/octe0jbk8fVVps3xjRyAZvopy3bRsvIUMb3s05YY0zjFpCJfvdB64Q1xpgjAjLRf7wig6ISZeIwm47YGGMCLtGrKtOXbWNYfGt6tGvmdjjGGOO6gEv0S5xO2InDrTZvjDEQgIl++rJ0WjQJ5Zx+Hd0OxRhj6oWASvR7DhYwL3kHlwzubJ2wxhjjCKhEP2tlJoUlHpuO2BhjSgmYRH+kEzahSyt6tbdOWGOMOcLXSwnWO3mFJQzr2poxPX9xoStjjGnUAibRR4WH8NSlJ7kdhjHG1DsB03RjjDGmfJbojTEmwFmiN8aYAGeJ3hhjApwlemOMCXCW6I0xJsBZojfGmABnid4YYwKcqKrbMfyCiOwCfvLx8DbA7hoIIxDLqU+xWDlWjttlBFo5XVS1bXkb6mWi94eIJKpqgpVTv2Oxcqwct8sI5HLKsqYbY4wJcJbojTEmwAVion/dyqnVMqwcK6c+lFOfYqmP5Rwj4NrojTHGHCsQa/TGGGNKsURvjDEBLiASvYjEisjXIrJeRNaJyG99LCdCRJaJSJJTzh/9jCtYRFaJyGd+lJEmImtFZLWIJPpRTksR+UhEUkRkg4iM9KGM3k4cR24HRORuH+O5x3mNk0VkuohE+FjOb50y1lUnFhGZIiLZIpJcal1rEVkgIqnOfSsfy7nciccjIlUaKldBOc86/681IvKJiLT0sZw/OWWsFpH5ItKpumWU2naviKiIVHoptwpieUJEMkudQxN8KcdZf5fz+qwTkWd8jGdGqVjSRGS1j+UMFJEfjrxPRWSYj+UMEJElznv+UxFpXlk5VaKqDf4GdAQGO8vNgE1AXx/KEaCpsxwKLAVG+BHX74BpwGd+lJEGtKmB1+ht4CZnOQxo6Wd5wcAOvD/SqO6xnYGtQBPn8UzgBh/K6QckA5F4r5b2JdCjiseeAgwGkkutewZ40Fl+EHjax3L6AL2Bb4AEP+IZB4Q4y0/7EU/zUsu/AV6tbhnO+lhgHt4fM1Z6TlYQyxPAfdX8P5dXzunO/zvcedzOl3LKbP878JiP8cwHznGWJwDf+FjOcuBUZ3kS8Kfqvi/KuwVEjV5Vt6vqSmc5F9iAN5lUtxxV1YPOw1Dn5lNvtYjEAOcCb/hyfE0SkRZ4T6o3AVS1UFVz/Cz2TOBHVfX1F8whQBMRCcGbqLN8KKMPsFRV81S1GPgWuKQqB6rqQmBvmdUX4v1AxLm/yJdyVHWDqm6sShyVlDPf+bsAfgBifCznQKmHUVRyTlfw2gD8E7i/suOrUE61VFDObcBTqlrg7JPtTzwiIsAVwHQfy1HgSO27BVU4nysopxew0FleAFxaWTlVERCJvjQRiQcG4a2N+3J8sPP1LRtYoKo+lQM8j/dN4fHx+CMUmC8iK0Rkso9ldAV2AW85TUlviEiUn3FdRRXeFOVR1UzgOWAbsB3Yr6rzfSgqGThZRKJFJBJvTSrWl5gc7VV1u7O8A2jvR1k1bRLwX18PFpG/iEg6cA3wmA/HXwhkqmqSrzGUcqfTlDSlKs1jFeiF93+/VES+FZGhfsZ0MrBTVVN9PP5u4FnnNX4OeMjHctbhrXAAXI5/5/PPAirRi0hT4GPg7jK1mCpT1RJVHYi39jRMRPr5EMd5QLaqrvAlhjLGqOpg4BzgDhE5xYcyQvB+Rfy3qg4CDuFtmvCJiIQBFwAf+nh8K7wnc1egExAlItdWtxxV3YC3SWM+8AWwGijxJaZyylZ8/DZX00TkEaAYeN/XMlT1EVWNdcq4s5rPHwk8jA8fEOX4N9AdGIj3Q/7vPpYTArQGRgC/B2Y6tXJfTcTHiovjNuAe5zW+B+fbsw8mAbeLyAq8zdCFfsT0s4BJ9CISijfJv6+qs/wtz2na+BoY78Pho4ELRCQN+AA4Q0Te8zGOTOc+G/gEqLSTpxwZQEapbycf4U38vjoHWKmqO308fiywVVV3qWoRMAsY5UtBqvqmqg5R1VOAfXj7Z3y1U0Q6Ajj3lTYH1DYRuQE4D7jG+fDx1/tUvzmgO94P5STnnI4BVopIh+o+uarudCpTHuD/8O18Bu85Pctpbl2G95tzpR3E5XGaDy8BZvgYC8D1eM9j8FaAfPq7VDVFVcep6hC8Hzw/+hHTzwIi0Tuf5G8CG1T1H36U0/bIyAYRaQKcBaRUtxxVfUhVY1Q1Hm8Tx1eqWu0aq4hEiUizI8t4O+d+MRKiCvHsANJFpLez6kxgfXXLKcXf2s82YISIRDr/uzPx9qtUm4i0c+7j8L5Zp/kR1xy8b1ic+//4UZbfRGQ83ua/C1Q1z49yepZ6eCHVPKdVda2qtlPVeOeczsA7+GGHD7F0LPXwYnw4nx2z8XbIIiK98A4w8HXWx7FAiqpm+Hg8eNvkT3WWzwB8agIqdT4HAX8AXvUjpqNqokfX7RswBu/X7DV4v76vBib4UM5JwCqnnGSq0ANfhTJPw8dRN0A3IMm5rQMe8SOOgUCi87fNBlr5WE4UsAdo4efr8ke8CScZeBdn9IQP5XyH90MrCTizGsdNx9t0UIQ3cd0IRAP/w/sm/RJo7WM5FzvLBcBOYJ6P5WwG0kud08cdLXOccj52Xuc1wKdA5+qWUWZ7GlUbdVNeLO8Ca51Y5gAdfSwnDHjP+btWAmf4Uo6zfipwq5/nzhhghXMeLgWG+FjOb/F+K90EPIUze4G/N5sCwRhjAlxANN0YY4ypmCV6Y4wJcJbojTEmwFmiN8aYAGeJ3hhjApwlemPwTp1RwWyNb4hIXzdiMqamhLgdgDH1mare5HYMxvjLavTGHBUiIu+Ld77+j5xf7n4jzpzyInLQmRwsyZl7vL2z/nLxzomfJCILj/8UxtQ9S/TGHNUb+Jeq9gEOALeX2R4F/KCqA/BOJXuzs/4x4Gxn/QV1FawxVWWJ3pij0lV1kbP8Ht6ftZdWCBy5WtgKIN5ZXgRMFZGb8V6QxZh6xRK9MUeVnQ+k7OMiPTpnSAlOH5eq3op3AqpYYIWIRNdqlMZUkyV6Y46Kk6PX0r0a+L4qB4lId1VdqqqP4b3AS41cLMKYmmKJ3pijNuK9uMsGoBXei2RUxbPOxZyTgcV4ZzA0pt6w2SuNMSbAWY3eGGMCnCV6Y4wJcJbojTEmwFmiN8aYAGeJ3hhjApwlemOMCXCW6I0xJsD9P1MTCu3xV8j1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ma.plotRmsePerBins(list(ma.binsRmse(1.0, model=SVR())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.plotRmsePerBins(list(ma.binsRmse(model=LinearSVR())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.compareMethods(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: ['logAge'] ~ [0, 1]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "Evaluating: ['logAge'] ~ [0, 1]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "Evaluating: ['logAge'] ~ [0, 1]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Evaluating: ['logAge'] ~ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ma.compareMethods(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.compareMethods(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from projects.auricular.ann import buildModel, evaluateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=1.0\n",
    "bins=10\n",
    "\n",
    "Y = pd.DataFrame([np.log(float(data1['age'])) for data1 in data]).values\n",
    "X = pd.DataFrame(ma.hist_descriptors[dist].getSampleHistogram2dData(bins, True, False)).values\n",
    "#X = pd.DataFrame(hist_descriptors[dist].getSampleHistogramData(bins)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"mylogs\")\n",
    "run_logdir = os.path.join(root_logdir, time.strftime(\"run_%Y_%m_%d_%H_%M_%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer ModuleWrapper has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/1000\n",
      "65/65 [==============================] - 1s 5ms/step - loss: 14.8503 - val_loss: 14.2992\n",
      "Epoch 2/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 13.9479 - val_loss: 13.3481\n",
      "Epoch 3/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 12.9303 - val_loss: 12.2502\n",
      "Epoch 4/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 11.7361 - val_loss: 10.9515\n",
      "Epoch 5/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 10.3085 - val_loss: 9.3865\n",
      "Epoch 6/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 8.6402 - val_loss: 7.6410\n",
      "Epoch 7/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 6.8165 - val_loss: 5.7790\n",
      "Epoch 8/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 4.9857 - val_loss: 4.0277\n",
      "Epoch 9/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 3.3476 - val_loss: 2.5623\n",
      "Epoch 10/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 2.0461 - val_loss: 1.4663\n",
      "Epoch 11/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 1.1372 - val_loss: 0.7673\n",
      "Epoch 12/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5970 - val_loss: 0.3910\n",
      "Epoch 13/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.2285\n",
      "Epoch 14/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2145 - val_loss: 0.1686\n",
      "Epoch 15/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1753 - val_loss: 0.1504\n",
      "Epoch 16/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1619 - val_loss: 0.1462\n",
      "Epoch 17/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1576 - val_loss: 0.1446\n",
      "Epoch 18/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1554 - val_loss: 0.1436\n",
      "Epoch 19/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1533 - val_loss: 0.1424\n",
      "Epoch 20/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1512 - val_loss: 0.1413\n",
      "Epoch 21/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1491 - val_loss: 0.1402\n",
      "Epoch 22/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1474 - val_loss: 0.1391\n",
      "Epoch 23/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1455 - val_loss: 0.1380\n",
      "Epoch 24/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1437 - val_loss: 0.1368\n",
      "Epoch 25/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1419 - val_loss: 0.1359\n",
      "Epoch 26/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1401 - val_loss: 0.1350\n",
      "Epoch 27/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1388 - val_loss: 0.1338\n",
      "Epoch 28/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1368 - val_loss: 0.1333\n",
      "Epoch 29/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1321\n",
      "Epoch 30/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1335 - val_loss: 0.1314\n",
      "Epoch 31/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1319 - val_loss: 0.1303\n",
      "Epoch 32/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1303 - val_loss: 0.1294\n",
      "Epoch 33/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1286 - val_loss: 0.1287\n",
      "Epoch 34/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1270 - val_loss: 0.1277\n",
      "Epoch 35/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1256 - val_loss: 0.1268\n",
      "Epoch 36/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1239 - val_loss: 0.1262\n",
      "Epoch 37/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.1253\n",
      "Epoch 38/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1211 - val_loss: 0.1249\n",
      "Epoch 39/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1198 - val_loss: 0.1240\n",
      "Epoch 40/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1185 - val_loss: 0.1231\n",
      "Epoch 41/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1173 - val_loss: 0.1226\n",
      "Epoch 42/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1162 - val_loss: 0.1220\n",
      "Epoch 43/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1152 - val_loss: 0.1214\n",
      "Epoch 44/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1141 - val_loss: 0.1208\n",
      "Epoch 45/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1133 - val_loss: 0.1203\n",
      "Epoch 46/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1123 - val_loss: 0.1198\n",
      "Epoch 47/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1114 - val_loss: 0.1196\n",
      "Epoch 48/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 0.1189\n",
      "Epoch 49/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1099 - val_loss: 0.1186\n",
      "Epoch 50/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1093 - val_loss: 0.1182\n",
      "Epoch 51/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1084 - val_loss: 0.1178\n",
      "Epoch 52/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1080 - val_loss: 0.1175\n",
      "Epoch 53/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1073 - val_loss: 0.1174\n",
      "Epoch 54/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1068 - val_loss: 0.1169\n",
      "Epoch 55/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1062 - val_loss: 0.1167\n",
      "Epoch 56/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.1164\n",
      "Epoch 57/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1053 - val_loss: 0.1163\n",
      "Epoch 58/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1049 - val_loss: 0.1159\n",
      "Epoch 59/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1045 - val_loss: 0.1157\n",
      "Epoch 60/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1043 - val_loss: 0.1156\n",
      "Epoch 61/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1037 - val_loss: 0.1153\n",
      "Epoch 62/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1041 - val_loss: 0.1153\n",
      "Epoch 63/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1036 - val_loss: 0.1150\n",
      "Epoch 64/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1030 - val_loss: 0.1148\n",
      "Epoch 65/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1028 - val_loss: 0.1147\n",
      "Epoch 66/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1028 - val_loss: 0.1145\n",
      "Epoch 67/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1021 - val_loss: 0.1145\n",
      "Epoch 68/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1019 - val_loss: 0.1143\n",
      "Epoch 69/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1019 - val_loss: 0.1141\n",
      "Epoch 70/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1016 - val_loss: 0.1140\n",
      "Epoch 71/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.1139\n",
      "Epoch 72/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1012 - val_loss: 0.1138\n",
      "Epoch 73/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1010 - val_loss: 0.1137\n",
      "Epoch 74/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1008 - val_loss: 0.1136\n",
      "Epoch 75/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1006 - val_loss: 0.1133\n",
      "Epoch 76/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1004 - val_loss: 0.1133\n",
      "Epoch 77/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1004 - val_loss: 0.1131\n",
      "Epoch 78/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1001 - val_loss: 0.1131\n",
      "Epoch 79/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0999 - val_loss: 0.1130\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0998 - val_loss: 0.1129\n",
      "Epoch 81/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0997 - val_loss: 0.1127\n",
      "Epoch 82/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0996 - val_loss: 0.1126\n",
      "Epoch 83/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0993 - val_loss: 0.1126\n",
      "Epoch 84/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0992 - val_loss: 0.1125\n",
      "Epoch 85/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0989 - val_loss: 0.1124\n",
      "Epoch 86/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0991 - val_loss: 0.1123\n",
      "Epoch 87/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0988 - val_loss: 0.1122\n",
      "Epoch 88/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0988 - val_loss: 0.1121\n",
      "Epoch 89/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0986 - val_loss: 0.1122\n",
      "Epoch 90/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0985 - val_loss: 0.1121\n",
      "Epoch 91/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0984 - val_loss: 0.1120\n",
      "Epoch 92/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0985 - val_loss: 0.1122\n",
      "Epoch 93/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0982 - val_loss: 0.1117\n",
      "Epoch 94/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0983 - val_loss: 0.1116\n",
      "Epoch 95/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0979 - val_loss: 0.1116\n",
      "Epoch 96/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0976 - val_loss: 0.1115\n",
      "Epoch 97/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0976 - val_loss: 0.1115\n",
      "Epoch 98/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0977 - val_loss: 0.1114\n",
      "Epoch 99/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0978 - val_loss: 0.1116\n",
      "Epoch 100/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0975 - val_loss: 0.1111\n",
      "Epoch 101/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0971 - val_loss: 0.1111\n",
      "Epoch 102/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0970 - val_loss: 0.1114\n",
      "Epoch 103/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0969 - val_loss: 0.1109\n",
      "Epoch 104/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.1109\n",
      "Epoch 105/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0968 - val_loss: 0.1109\n",
      "Epoch 106/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.1108\n",
      "Epoch 107/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0965 - val_loss: 0.1106\n",
      "Epoch 108/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0965 - val_loss: 0.1106\n",
      "Epoch 109/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0963 - val_loss: 0.1106\n",
      "Epoch 110/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0963 - val_loss: 0.1104\n",
      "Epoch 111/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0962 - val_loss: 0.1104\n",
      "Epoch 112/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0962 - val_loss: 0.1103\n",
      "Epoch 113/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0959 - val_loss: 0.1105\n",
      "Epoch 114/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0961 - val_loss: 0.1106\n",
      "Epoch 115/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0957 - val_loss: 0.1101\n",
      "Epoch 116/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0954 - val_loss: 0.1101\n",
      "Epoch 117/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0956 - val_loss: 0.1103\n",
      "Epoch 118/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0959 - val_loss: 0.1105\n",
      "Epoch 119/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0953 - val_loss: 0.1099\n",
      "Epoch 120/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0962 - val_loss: 0.1101\n",
      "Epoch 121/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0952 - val_loss: 0.1098\n",
      "Epoch 122/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0949 - val_loss: 0.1098\n",
      "Epoch 123/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0960 - val_loss: 0.1101\n",
      "Epoch 124/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0952 - val_loss: 0.1097\n",
      "Epoch 125/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0948 - val_loss: 0.1095\n",
      "Epoch 126/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0951 - val_loss: 0.1095\n",
      "Epoch 127/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0948 - val_loss: 0.1095\n",
      "Epoch 128/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0947 - val_loss: 0.1096\n",
      "Epoch 129/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0947 - val_loss: 0.1093\n",
      "Epoch 130/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0942 - val_loss: 0.1093\n",
      "Epoch 131/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0942 - val_loss: 0.1092\n",
      "Epoch 132/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0944 - val_loss: 0.1091\n",
      "Epoch 133/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0948 - val_loss: 0.1091\n",
      "Epoch 134/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0941 - val_loss: 0.1090\n",
      "Epoch 135/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.1093\n",
      "Epoch 136/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0939 - val_loss: 0.1089\n",
      "Epoch 137/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.1088\n",
      "Epoch 138/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.1092\n",
      "Epoch 139/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0936 - val_loss: 0.1089\n",
      "Epoch 140/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.1088\n",
      "Epoch 141/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0934 - val_loss: 0.1086\n",
      "Epoch 142/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0932 - val_loss: 0.1091\n",
      "Epoch 143/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0936 - val_loss: 0.1088\n",
      "Epoch 144/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0934 - val_loss: 0.1085\n",
      "Epoch 145/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0935 - val_loss: 0.1085\n",
      "Epoch 146/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0938 - val_loss: 0.1094\n",
      "Epoch 147/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.1085\n",
      "Epoch 148/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0932 - val_loss: 0.1083\n",
      "Epoch 149/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0928 - val_loss: 0.1082\n",
      "Epoch 150/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0929 - val_loss: 0.1081\n",
      "Epoch 151/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.1081\n",
      "Epoch 152/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.1084\n",
      "Epoch 153/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.1081\n",
      "Epoch 154/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0926 - val_loss: 0.1083\n",
      "Epoch 155/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.1082\n",
      "Epoch 156/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.1078\n",
      "Epoch 157/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0936 - val_loss: 0.1084\n",
      "Epoch 158/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0931 - val_loss: 0.1078\n",
      "Epoch 159/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0923 - val_loss: 0.1081\n",
      "Epoch 160/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0924 - val_loss: 0.1078\n",
      "Epoch 161/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0924 - val_loss: 0.1079\n",
      "Epoch 162/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0923 - val_loss: 0.1077\n",
      "Epoch 163/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1075\n",
      "Epoch 164/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0922 - val_loss: 0.1076\n",
      "Epoch 165/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0919 - val_loss: 0.1076\n",
      "Epoch 166/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1076\n",
      "Epoch 167/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1077\n",
      "Epoch 168/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1076\n",
      "Epoch 169/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1073\n",
      "Epoch 170/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1072\n",
      "Epoch 171/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0921 - val_loss: 0.1076\n",
      "Epoch 172/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0923 - val_loss: 0.1075\n",
      "Epoch 173/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0917 - val_loss: 0.1073\n",
      "Epoch 174/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0916 - val_loss: 0.1070\n",
      "Epoch 175/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0916 - val_loss: 0.1069\n",
      "Epoch 176/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0914 - val_loss: 0.1070\n",
      "Epoch 177/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0916 - val_loss: 0.1069\n",
      "Epoch 178/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.1074\n",
      "Epoch 179/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0919 - val_loss: 0.1068\n",
      "Epoch 180/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1069\n",
      "Epoch 181/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1067\n",
      "Epoch 182/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0914 - val_loss: 0.1069\n",
      "Epoch 183/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0916 - val_loss: 0.1066\n",
      "Epoch 184/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1066\n",
      "Epoch 185/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1068\n",
      "Epoch 186/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0910 - val_loss: 0.1065\n",
      "Epoch 187/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1065\n",
      "Epoch 188/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1066\n",
      "Epoch 189/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0910 - val_loss: 0.1066\n",
      "Epoch 190/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0908 - val_loss: 0.1066\n",
      "Epoch 191/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0910 - val_loss: 0.1063\n",
      "Epoch 192/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0913 - val_loss: 0.1064\n",
      "Epoch 193/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.1073\n",
      "Epoch 194/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0906 - val_loss: 0.1061\n",
      "Epoch 195/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0907 - val_loss: 0.1063\n",
      "Epoch 196/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0909 - val_loss: 0.1067\n",
      "Epoch 197/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0907 - val_loss: 0.1061\n",
      "Epoch 198/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0905 - val_loss: 0.1059\n",
      "Epoch 199/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.1060\n",
      "Epoch 200/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0904 - val_loss: 0.1058\n",
      "Epoch 201/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0909 - val_loss: 0.1060\n",
      "Epoch 202/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0913 - val_loss: 0.1057\n",
      "Epoch 203/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0907 - val_loss: 0.1057\n",
      "Epoch 204/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0909 - val_loss: 0.1057\n",
      "Epoch 205/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.1056\n",
      "Epoch 206/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1061\n",
      "Epoch 207/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.1056\n",
      "Epoch 208/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0900 - val_loss: 0.1057\n",
      "Epoch 209/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.1056\n",
      "Epoch 210/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1056\n",
      "Epoch 211/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0900 - val_loss: 0.1069\n",
      "Epoch 212/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0900 - val_loss: 0.1053\n",
      "Epoch 213/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1055\n",
      "Epoch 214/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1053\n",
      "Epoch 215/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0899 - val_loss: 0.1055\n",
      "Epoch 216/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0899 - val_loss: 0.1052\n",
      "Epoch 217/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0897 - val_loss: 0.1062\n",
      "Epoch 218/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0900 - val_loss: 0.1051\n",
      "Epoch 219/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0902 - val_loss: 0.1082\n",
      "Epoch 220/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0906 - val_loss: 0.1051\n",
      "Epoch 221/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0897 - val_loss: 0.1051\n",
      "Epoch 222/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0898 - val_loss: 0.1049\n",
      "Epoch 223/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0898 - val_loss: 0.1049\n",
      "Epoch 224/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0906 - val_loss: 0.1050\n",
      "Epoch 225/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.1049\n",
      "Epoch 226/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.1051\n",
      "Epoch 227/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0899 - val_loss: 0.1052\n",
      "Epoch 228/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.1053\n",
      "Epoch 229/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.1053\n",
      "Epoch 230/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0902 - val_loss: 0.1059\n",
      "Epoch 231/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1046\n",
      "Epoch 232/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0895 - val_loss: 0.1050\n",
      "Epoch 233/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 0.1047\n",
      "Epoch 234/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0895 - val_loss: 0.1045\n",
      "Epoch 235/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.1045\n",
      "Epoch 236/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1046\n",
      "Epoch 237/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0897 - val_loss: 0.1047\n",
      "Epoch 238/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.1048\n",
      "Epoch 239/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0892 - val_loss: 0.1044\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1066\n",
      "Epoch 241/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0898 - val_loss: 0.1044\n",
      "Epoch 242/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1045\n",
      "Epoch 243/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1044\n",
      "Epoch 244/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.1047\n",
      "Epoch 245/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0892 - val_loss: 0.1042\n",
      "Epoch 246/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0890 - val_loss: 0.1041\n",
      "Epoch 247/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 0.1053\n",
      "Epoch 248/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0890 - val_loss: 0.1044\n",
      "Epoch 249/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.1040\n",
      "Epoch 250/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0889 - val_loss: 0.1045\n",
      "Epoch 251/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0888 - val_loss: 0.1039\n",
      "Epoch 252/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.1042\n",
      "Epoch 253/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.1038\n",
      "Epoch 254/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0890 - val_loss: 0.1043\n",
      "Epoch 255/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0889 - val_loss: 0.1045\n",
      "Epoch 256/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0889 - val_loss: 0.1044\n",
      "Epoch 257/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0892 - val_loss: 0.1038\n",
      "Epoch 258/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0893 - val_loss: 0.1048\n",
      "Epoch 259/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0909 - val_loss: 0.1063\n",
      "Epoch 260/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.1042\n",
      "Epoch 261/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.1037\n",
      "Epoch 262/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1046\n",
      "Epoch 263/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0888 - val_loss: 0.1038\n",
      "Epoch 264/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.1038\n",
      "Epoch 265/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1043\n",
      "Epoch 266/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.1036\n",
      "Epoch 267/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.1035\n",
      "Epoch 268/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0888 - val_loss: 0.1039\n",
      "Epoch 269/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0890 - val_loss: 0.1034\n",
      "Epoch 270/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0884 - val_loss: 0.1034\n",
      "Epoch 271/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0892 - val_loss: 0.1033\n",
      "Epoch 272/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1033\n",
      "Epoch 273/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0883 - val_loss: 0.1047\n",
      "Epoch 274/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.1033\n",
      "Epoch 275/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0884 - val_loss: 0.1043\n",
      "Epoch 276/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1031\n",
      "Epoch 277/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1035\n",
      "Epoch 278/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.1041\n",
      "Epoch 279/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1030\n",
      "Epoch 280/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1038\n",
      "Epoch 281/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0883 - val_loss: 0.1031\n",
      "Epoch 282/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.1031\n",
      "Epoch 283/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0883 - val_loss: 0.1030\n",
      "Epoch 284/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0884 - val_loss: 0.1029\n",
      "Epoch 285/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0883 - val_loss: 0.1032\n",
      "Epoch 286/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0879 - val_loss: 0.1030\n",
      "Epoch 287/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.1033\n",
      "Epoch 288/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1028\n",
      "Epoch 289/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.1036\n",
      "Epoch 290/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0879 - val_loss: 0.1028\n",
      "Epoch 291/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0880 - val_loss: 0.1027\n",
      "Epoch 292/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0880 - val_loss: 0.1030\n",
      "Epoch 293/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0879 - val_loss: 0.1030\n",
      "Epoch 294/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.1026\n",
      "Epoch 295/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1026\n",
      "Epoch 296/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - val_loss: 0.1026\n",
      "Epoch 297/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0880 - val_loss: 0.1030\n",
      "Epoch 298/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1044\n",
      "Epoch 299/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1026\n",
      "Epoch 300/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1027\n",
      "Epoch 301/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0879 - val_loss: 0.1027\n",
      "Epoch 302/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.1025\n",
      "Epoch 303/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.1024\n",
      "Epoch 304/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1024\n",
      "Epoch 305/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0892 - val_loss: 0.1029\n",
      "Epoch 306/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0889 - val_loss: 0.1025\n",
      "Epoch 307/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1029\n",
      "Epoch 308/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1023\n",
      "Epoch 309/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0880 - val_loss: 0.1033\n",
      "Epoch 310/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 0.1023\n",
      "Epoch 311/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1023\n",
      "Epoch 312/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1025\n",
      "Epoch 313/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1022\n",
      "Epoch 314/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1021\n",
      "Epoch 315/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1021\n",
      "Epoch 316/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1028\n",
      "Epoch 317/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1023\n",
      "Epoch 318/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1020\n",
      "Epoch 319/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 0.1020\n",
      "Epoch 320/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1041\n",
      "Epoch 321/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1021\n",
      "Epoch 322/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 0.1019\n",
      "Epoch 323/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1026\n",
      "Epoch 324/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1019\n",
      "Epoch 325/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1030\n",
      "Epoch 326/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1018\n",
      "Epoch 327/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1023\n",
      "Epoch 328/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1020\n",
      "Epoch 329/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1023\n",
      "Epoch 330/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1025\n",
      "Epoch 331/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.1018\n",
      "Epoch 332/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - val_loss: 0.1018\n",
      "Epoch 333/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.1016\n",
      "Epoch 334/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1015\n",
      "Epoch 335/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.1023\n",
      "Epoch 336/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0876 - val_loss: 0.1025\n",
      "Epoch 337/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1028\n",
      "Epoch 338/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1017\n",
      "Epoch 339/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1017\n",
      "Epoch 340/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1014\n",
      "Epoch 341/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.1014\n",
      "Epoch 342/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 0.1028\n",
      "Epoch 343/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 0.1018\n",
      "Epoch 344/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1013\n",
      "Epoch 345/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 0.1013\n",
      "Epoch 346/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.1040\n",
      "Epoch 347/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1013\n",
      "Epoch 348/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1017\n",
      "Epoch 349/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1018\n",
      "Epoch 350/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1012\n",
      "Epoch 351/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1013\n",
      "Epoch 352/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1020\n",
      "Epoch 353/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1011\n",
      "Epoch 354/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1018\n",
      "Epoch 355/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1011\n",
      "Epoch 356/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1010\n",
      "Epoch 357/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.1010\n",
      "Epoch 358/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0869 - val_loss: 0.1017\n",
      "Epoch 359/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1014\n",
      "Epoch 360/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1011\n",
      "Epoch 361/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1019\n",
      "Epoch 362/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1009\n",
      "Epoch 363/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 0.1012\n",
      "Epoch 364/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0877 - val_loss: 0.1023\n",
      "Epoch 365/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1021\n",
      "Epoch 366/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1009\n",
      "Epoch 367/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1008\n",
      "Epoch 368/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1009\n",
      "Epoch 369/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.1020\n",
      "Epoch 370/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.1008\n",
      "Epoch 371/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1030\n",
      "Epoch 372/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0869 - val_loss: 0.1010\n",
      "Epoch 373/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1009\n",
      "Epoch 374/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1012\n",
      "Epoch 375/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1010\n",
      "Epoch 376/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1028\n",
      "Epoch 377/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.1006\n",
      "Epoch 378/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1030\n",
      "Epoch 379/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0870 - val_loss: 0.1008\n",
      "Epoch 380/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.1012\n",
      "Epoch 381/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.1010\n",
      "Epoch 382/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1006\n",
      "Epoch 383/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.1015\n",
      "Epoch 384/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1008\n",
      "Epoch 385/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1016\n",
      "Epoch 386/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1010\n",
      "Epoch 387/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1013\n",
      "Epoch 388/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1009\n",
      "Epoch 389/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1006\n",
      "Epoch 390/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1005\n",
      "Epoch 391/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.1004\n",
      "Epoch 392/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.1004\n",
      "Epoch 393/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1055\n",
      "Epoch 394/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1004\n",
      "Epoch 395/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.1012\n",
      "Epoch 396/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.1004\n",
      "Epoch 397/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.1007\n",
      "Epoch 398/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1005\n",
      "Epoch 399/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.1002\n",
      "Epoch 400/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0869 - val_loss: 0.1004\n",
      "Epoch 401/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0875 - val_loss: 0.1002\n",
      "Epoch 402/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1004\n",
      "Epoch 403/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.1002\n",
      "Epoch 404/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1012\n",
      "Epoch 405/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1011\n",
      "Epoch 406/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.1016\n",
      "Epoch 407/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1007\n",
      "Epoch 408/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1004\n",
      "Epoch 409/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1001\n",
      "Epoch 410/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1000\n",
      "Epoch 411/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.1014\n",
      "Epoch 412/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.1000\n",
      "Epoch 413/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1003\n",
      "Epoch 414/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1001\n",
      "Epoch 415/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.1016\n",
      "Epoch 416/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1000\n",
      "Epoch 417/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0869 - val_loss: 0.0999\n",
      "Epoch 418/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1001\n",
      "Epoch 419/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.1006\n",
      "Epoch 420/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.0999\n",
      "Epoch 421/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.0998\n",
      "Epoch 422/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1002\n",
      "Epoch 423/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.0998\n",
      "Epoch 424/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.1019\n",
      "Epoch 425/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.0998\n",
      "Epoch 426/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1009\n",
      "Epoch 427/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.1013\n",
      "Epoch 428/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1003\n",
      "Epoch 429/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1021\n",
      "Epoch 430/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.0997\n",
      "Epoch 431/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0997\n",
      "Epoch 432/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.1001\n",
      "Epoch 433/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0868 - val_loss: 0.1027\n",
      "Epoch 434/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1005\n",
      "Epoch 435/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 0.1019\n",
      "Epoch 436/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1001\n",
      "Epoch 437/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.0996\n",
      "Epoch 438/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.1018\n",
      "Epoch 439/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.1002\n",
      "Epoch 440/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.1003\n",
      "Epoch 441/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0998\n",
      "Epoch 442/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0995\n",
      "Epoch 443/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.1002\n",
      "Epoch 444/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.1002\n",
      "Epoch 445/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1000\n",
      "Epoch 446/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0996\n",
      "Epoch 447/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.1001\n",
      "Epoch 448/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1001\n",
      "Epoch 449/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.0996\n",
      "Epoch 450/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0994\n",
      "Epoch 451/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - val_loss: 0.1010\n",
      "Epoch 452/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0996\n",
      "Epoch 453/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.0996\n",
      "Epoch 454/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.1008\n",
      "Epoch 455/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.1025\n",
      "Epoch 456/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0995\n",
      "Epoch 457/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.0998\n",
      "Epoch 458/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.0996\n",
      "Epoch 459/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0994\n",
      "Epoch 460/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.1001\n",
      "Epoch 461/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0859 - val_loss: 0.1005\n",
      "Epoch 462/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.1004\n",
      "Epoch 463/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0993\n",
      "Epoch 464/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0996\n",
      "Epoch 465/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0993\n",
      "Epoch 466/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.1008\n",
      "Epoch 467/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.0996\n",
      "Epoch 468/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.0995\n",
      "Epoch 469/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.1015\n",
      "Epoch 470/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0991\n",
      "Epoch 471/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.0999\n",
      "Epoch 472/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.0991\n",
      "Epoch 473/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.0993\n",
      "Epoch 474/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.0993\n",
      "Epoch 475/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0995\n",
      "Epoch 476/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.0996\n",
      "Epoch 477/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0992\n",
      "Epoch 478/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.1012\n",
      "Epoch 479/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.1002\n",
      "Epoch 480/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.0993\n",
      "Epoch 481/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.0990\n",
      "Epoch 482/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0999\n",
      "Epoch 483/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0997\n",
      "Epoch 484/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0993\n",
      "Epoch 485/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.0989\n",
      "Epoch 486/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0858 - val_loss: 0.0993\n",
      "Epoch 487/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0999\n",
      "Epoch 488/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.0989\n",
      "Epoch 489/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0998\n",
      "Epoch 490/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0990\n",
      "Epoch 491/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.0999\n",
      "Epoch 492/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0988\n",
      "Epoch 493/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 0.0988\n",
      "Epoch 494/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0989\n",
      "Epoch 495/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.0988\n",
      "Epoch 496/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.1016\n",
      "Epoch 497/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0989\n",
      "Epoch 498/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0987\n",
      "Epoch 499/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0987\n",
      "Epoch 500/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0987\n",
      "Epoch 501/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0995\n",
      "Epoch 502/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0987\n",
      "Epoch 503/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0992\n",
      "Epoch 504/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0999\n",
      "Epoch 505/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0989\n",
      "Epoch 506/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0986\n",
      "Epoch 507/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0986\n",
      "Epoch 508/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0988\n",
      "Epoch 509/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0999\n",
      "Epoch 510/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0993\n",
      "Epoch 511/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0996\n",
      "Epoch 512/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0987\n",
      "Epoch 513/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0986\n",
      "Epoch 514/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0999\n",
      "Epoch 515/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0999\n",
      "Epoch 516/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0986\n",
      "Epoch 517/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0992\n",
      "Epoch 518/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0990\n",
      "Epoch 519/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0988\n",
      "Epoch 520/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0994\n",
      "Epoch 521/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0985\n",
      "Epoch 522/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0985\n",
      "Epoch 523/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0989\n",
      "Epoch 524/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0992\n",
      "Epoch 525/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0991\n",
      "Epoch 526/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.1008\n",
      "Epoch 527/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0986\n",
      "Epoch 528/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0984\n",
      "Epoch 529/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0991\n",
      "Epoch 530/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 0.0987\n",
      "Epoch 531/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - val_loss: 0.0986\n",
      "Epoch 532/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0993\n",
      "Epoch 533/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0991\n",
      "Epoch 534/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0990\n",
      "Epoch 535/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0987\n",
      "Epoch 536/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0990\n",
      "Epoch 537/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0998\n",
      "Epoch 538/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - val_loss: 0.0988\n",
      "Epoch 539/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0984\n",
      "Epoch 540/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0989\n",
      "Epoch 541/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0988\n",
      "Epoch 542/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0983\n",
      "Epoch 543/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0988\n",
      "Epoch 544/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0983\n",
      "Epoch 545/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0984\n",
      "Epoch 546/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0983\n",
      "Epoch 547/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0983\n",
      "Epoch 548/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0995\n",
      "Epoch 549/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0983\n",
      "Epoch 550/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.0984\n",
      "Epoch 551/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0985\n",
      "Epoch 552/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0987\n",
      "Epoch 553/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0984\n",
      "Epoch 554/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0994\n",
      "Epoch 555/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0992\n",
      "Epoch 556/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0983\n",
      "Epoch 557/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0995\n",
      "Epoch 558/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0982\n",
      "Epoch 559/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0985\n",
      "Epoch 560/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0982\n",
      "Epoch 561/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0984\n",
      "Epoch 562/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.1000\n",
      "Epoch 563/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0991\n",
      "Epoch 564/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.1013\n",
      "Epoch 565/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0988\n",
      "Epoch 566/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0983\n",
      "Epoch 567/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.1017\n",
      "Epoch 568/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0982\n",
      "Epoch 569/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0989\n",
      "Epoch 570/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.1001\n",
      "Epoch 571/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0985\n",
      "Epoch 572/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0982\n",
      "Epoch 573/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.1001\n",
      "Epoch 574/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.0993\n",
      "Epoch 575/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0989\n",
      "Epoch 576/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0983\n",
      "Epoch 577/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.0985\n",
      "Epoch 578/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0982\n",
      "Epoch 579/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0982\n",
      "Epoch 580/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0984\n",
      "Epoch 581/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0990\n",
      "Epoch 582/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0984\n",
      "Epoch 583/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0984\n",
      "Epoch 584/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0983\n",
      "Epoch 585/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0994\n",
      "Epoch 586/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0982\n",
      "Epoch 587/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0981\n",
      "Epoch 588/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0989\n",
      "Epoch 589/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0999\n",
      "Epoch 590/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0992\n",
      "Epoch 591/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0986\n",
      "Epoch 592/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0981\n",
      "Epoch 593/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.0981\n",
      "Epoch 594/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0851 - val_loss: 0.0980\n",
      "Epoch 595/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0981\n",
      "Epoch 596/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0986\n",
      "Epoch 597/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0845 - val_loss: 0.0984\n",
      "Epoch 598/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0992\n",
      "Epoch 599/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.1000\n",
      "Epoch 600/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0982\n",
      "Epoch 601/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0982\n",
      "Epoch 602/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0987\n",
      "Epoch 603/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0986\n",
      "Epoch 604/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.0987\n",
      "Epoch 605/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0981\n",
      "Epoch 606/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0843 - val_loss: 0.0980\n",
      "Epoch 607/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0989\n",
      "Epoch 608/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0990\n",
      "Epoch 609/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0844 - val_loss: 0.0982\n",
      "Epoch 610/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0983\n",
      "Epoch 611/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0991\n",
      "Epoch 612/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0990\n",
      "Epoch 613/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0983\n",
      "Epoch 614/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0982\n",
      "Epoch 615/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0980\n",
      "Epoch 616/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.1005\n",
      "Epoch 617/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0982\n",
      "Epoch 618/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0980\n",
      "Epoch 619/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0980\n",
      "Epoch 620/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0980\n",
      "Epoch 621/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0979\n",
      "Epoch 622/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0839 - val_loss: 0.0980\n",
      "Epoch 623/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0980\n",
      "Epoch 624/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0841 - val_loss: 0.0983\n",
      "Epoch 625/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0984\n",
      "Epoch 626/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0983\n",
      "Epoch 627/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.1001\n",
      "Epoch 628/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0992\n",
      "Epoch 629/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0984\n",
      "Epoch 630/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0978\n",
      "Epoch 631/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.1008\n",
      "Epoch 632/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0981\n",
      "Epoch 633/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0980\n",
      "Epoch 634/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0986\n",
      "Epoch 635/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0983\n",
      "Epoch 636/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0982\n",
      "Epoch 637/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0980\n",
      "Epoch 638/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.1005\n",
      "Epoch 639/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0980\n",
      "Epoch 640/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0993\n",
      "Epoch 641/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0981\n",
      "Epoch 642/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0982\n",
      "Epoch 643/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0998\n",
      "Epoch 644/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0988\n",
      "Epoch 645/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0977\n",
      "Epoch 646/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0978\n",
      "Epoch 647/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0977\n",
      "Epoch 648/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.1003\n",
      "Epoch 649/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0985\n",
      "Epoch 650/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0981\n",
      "Epoch 651/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0981\n",
      "Epoch 652/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0982\n",
      "Epoch 653/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0984\n",
      "Epoch 654/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.1008\n",
      "Epoch 655/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0987\n",
      "Epoch 656/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0980\n",
      "Epoch 657/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0978\n",
      "Epoch 658/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0982\n",
      "Epoch 659/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0993\n",
      "Epoch 660/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0982\n",
      "Epoch 661/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0978\n",
      "Epoch 662/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0990\n",
      "Epoch 663/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0984\n",
      "Epoch 664/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0981\n",
      "Epoch 665/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0977\n",
      "Epoch 666/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0988\n",
      "Epoch 667/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0977\n",
      "Epoch 668/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0979\n",
      "Epoch 669/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0977\n",
      "Epoch 670/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0979\n",
      "Epoch 671/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0985\n",
      "Epoch 672/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0976\n",
      "Epoch 673/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0991\n",
      "Epoch 674/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0976\n",
      "Epoch 675/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0983\n",
      "Epoch 676/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0979\n",
      "Epoch 677/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0980\n",
      "Epoch 678/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0991\n",
      "Epoch 679/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0985\n",
      "Epoch 680/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0837 - val_loss: 0.0996\n",
      "Epoch 681/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0981\n",
      "Epoch 682/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0991\n",
      "Epoch 683/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 0.0978\n",
      "Epoch 684/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.1010\n",
      "Epoch 685/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0978\n",
      "Epoch 686/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0990\n",
      "Epoch 687/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0989\n",
      "Epoch 688/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0981\n",
      "Epoch 689/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0983\n",
      "Epoch 690/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0984\n",
      "Epoch 691/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0833 - val_loss: 0.0976\n",
      "Epoch 692/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0983\n",
      "Epoch 693/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0981\n",
      "Epoch 694/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.1001\n",
      "Epoch 695/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0840 - val_loss: 0.0986\n",
      "Epoch 696/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0996\n",
      "Epoch 697/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0981\n",
      "Epoch 698/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0984\n",
      "Epoch 699/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0976\n",
      "Epoch 700/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0986\n",
      "Epoch 701/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0980\n",
      "Epoch 702/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0981\n",
      "Epoch 703/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0976\n",
      "Epoch 704/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0828 - val_loss: 0.0975\n",
      "Epoch 705/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0977\n",
      "Epoch 706/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0979\n",
      "Epoch 707/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0991\n",
      "Epoch 708/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0977\n",
      "Epoch 709/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0988\n",
      "Epoch 710/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0982\n",
      "Epoch 711/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0977\n",
      "Epoch 712/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0978\n",
      "Epoch 713/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0977\n",
      "Epoch 714/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.1004\n",
      "Epoch 715/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.0979\n",
      "Epoch 716/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0985\n",
      "Epoch 717/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0975\n",
      "Epoch 718/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0975\n",
      "Epoch 719/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0982\n",
      "Epoch 720/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0982\n",
      "Epoch 721/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0976\n",
      "Epoch 722/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.1010\n",
      "Epoch 723/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0976\n",
      "Epoch 724/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.0975\n",
      "Epoch 725/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0989\n",
      "Epoch 726/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0975\n",
      "Epoch 727/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0838 - val_loss: 0.0973\n",
      "Epoch 728/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0980\n",
      "Epoch 729/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0977\n",
      "Epoch 730/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0989\n",
      "Epoch 731/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - val_loss: 0.0978\n",
      "Epoch 732/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0976\n",
      "Epoch 733/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.0979\n",
      "Epoch 734/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.1001\n",
      "Epoch 735/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0974\n",
      "Epoch 736/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0977\n",
      "Epoch 737/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0993\n",
      "Epoch 738/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0990\n",
      "Epoch 739/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0979\n",
      "Epoch 740/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0981\n",
      "Epoch 741/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0976\n",
      "Epoch 742/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0981\n",
      "Epoch 743/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0978\n",
      "Epoch 744/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0974\n",
      "Epoch 745/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0975\n",
      "Epoch 746/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0974\n",
      "Epoch 747/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0827 - val_loss: 0.0977\n",
      "Epoch 748/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0979\n",
      "Epoch 749/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0974\n",
      "Epoch 750/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0993\n",
      "Epoch 751/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0973\n",
      "Epoch 752/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0999\n",
      "Epoch 753/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0974\n",
      "Epoch 754/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0823 - val_loss: 0.0975\n",
      "Epoch 755/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.1004\n",
      "Epoch 756/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.0975\n",
      "Epoch 757/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.0973\n",
      "Epoch 758/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0978\n",
      "Epoch 759/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0994\n",
      "Epoch 760/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0982\n",
      "Epoch 761/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0975\n",
      "Epoch 762/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0986\n",
      "Epoch 763/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0979\n",
      "Epoch 764/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0988\n",
      "Epoch 765/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0980\n",
      "Epoch 766/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0976\n",
      "Epoch 767/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0979\n",
      "Epoch 768/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0980\n",
      "Epoch 769/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0977\n",
      "Epoch 770/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0978\n",
      "Epoch 771/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0987\n",
      "Epoch 772/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0974\n",
      "Epoch 773/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0973\n",
      "Epoch 774/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0973\n",
      "Epoch 775/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0978\n",
      "Epoch 776/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0985\n",
      "Epoch 777/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0974\n",
      "Epoch 778/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0978\n",
      "Epoch 779/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0999\n",
      "Epoch 780/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0972\n",
      "Epoch 781/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0973\n",
      "Epoch 782/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0986\n",
      "Epoch 783/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0986\n",
      "Epoch 784/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0984\n",
      "Epoch 785/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0972\n",
      "Epoch 786/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0976\n",
      "Epoch 787/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0982\n",
      "Epoch 788/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0974\n",
      "Epoch 789/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0972\n",
      "Epoch 790/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0972\n",
      "Epoch 791/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0975\n",
      "Epoch 792/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0979\n",
      "Epoch 793/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0971\n",
      "Epoch 794/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0976\n",
      "Epoch 795/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0991\n",
      "Epoch 796/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0976\n",
      "Epoch 797/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0979\n",
      "Epoch 798/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0972\n",
      "Epoch 799/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0981\n",
      "Epoch 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0989\n",
      "Epoch 801/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0971\n",
      "Epoch 802/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0971\n",
      "Epoch 803/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0973\n",
      "Epoch 804/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0978\n",
      "Epoch 805/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0980\n",
      "Epoch 806/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0989\n",
      "Epoch 807/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0971\n",
      "Epoch 808/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - val_loss: 0.0978\n",
      "Epoch 809/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0971\n",
      "Epoch 810/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0982\n",
      "Epoch 811/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0977\n",
      "Epoch 812/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0975\n",
      "Epoch 813/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0973\n",
      "Epoch 814/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0990\n",
      "Epoch 815/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0995\n",
      "Epoch 816/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - val_loss: 0.0973\n",
      "Epoch 817/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0984\n",
      "Epoch 818/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0976\n",
      "Epoch 819/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0971\n",
      "Epoch 820/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0974\n",
      "Epoch 821/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0970\n",
      "Epoch 822/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0982\n",
      "Epoch 823/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0972\n",
      "Epoch 824/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0812 - val_loss: 0.0973\n",
      "Epoch 825/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0975\n",
      "Epoch 826/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0973\n",
      "Epoch 827/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0979\n",
      "Epoch 828/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0984\n",
      "Epoch 829/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0974\n",
      "Epoch 830/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0970\n",
      "Epoch 831/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0982\n",
      "Epoch 832/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0980\n",
      "Epoch 833/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0976\n",
      "Epoch 834/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - val_loss: 0.0984\n",
      "Epoch 835/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0989\n",
      "Epoch 836/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0977\n",
      "Epoch 837/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0981\n",
      "Epoch 838/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0982\n",
      "Epoch 839/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0971\n",
      "Epoch 840/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0988\n",
      "Epoch 841/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0994\n",
      "Epoch 842/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0972\n",
      "Epoch 843/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0972\n",
      "Epoch 844/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.0969\n",
      "Epoch 845/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0980\n",
      "Epoch 846/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0987\n",
      "Epoch 847/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0971\n",
      "Epoch 848/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0970\n",
      "Epoch 849/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - val_loss: 0.0971\n",
      "Epoch 850/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0979\n",
      "Epoch 851/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0970\n",
      "Epoch 852/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0979\n",
      "Epoch 853/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0972\n",
      "Epoch 854/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0975\n",
      "Epoch 855/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0972\n",
      "Epoch 856/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0996\n",
      "Epoch 857/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0973\n",
      "Epoch 858/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0975\n",
      "Epoch 859/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0972\n",
      "Epoch 860/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0969\n",
      "Epoch 861/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0969\n",
      "Epoch 862/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.0986\n",
      "Epoch 863/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0969\n",
      "Epoch 864/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0975\n",
      "Epoch 865/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0970\n",
      "Epoch 866/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0977\n",
      "Epoch 867/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0979\n",
      "Epoch 868/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0974\n",
      "Epoch 869/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0969\n",
      "Epoch 870/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0980\n",
      "Epoch 871/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0974\n",
      "Epoch 872/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0980\n",
      "Epoch 873/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0970\n",
      "Epoch 874/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0999\n",
      "Epoch 875/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0986\n",
      "Epoch 876/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0969\n",
      "Epoch 877/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0812 - val_loss: 0.0975\n",
      "Epoch 878/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0978\n",
      "Epoch 879/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0994\n",
      "Epoch 880/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0971\n",
      "Epoch 881/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0974\n",
      "Epoch 882/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0972\n",
      "Epoch 883/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.1004\n",
      "Epoch 884/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0971\n",
      "Epoch 885/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0971\n",
      "Epoch 886/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0994\n",
      "Epoch 887/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0968\n",
      "Epoch 888/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0812 - val_loss: 0.0976\n",
      "Epoch 889/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0972\n",
      "Epoch 890/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0988\n",
      "Epoch 891/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0980\n",
      "Epoch 892/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0970\n",
      "Epoch 893/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0971\n",
      "Epoch 894/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0974\n",
      "Epoch 895/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.1019\n",
      "Epoch 896/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0992\n",
      "Epoch 897/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0982\n",
      "Epoch 898/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0987\n",
      "Epoch 899/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0968\n",
      "Epoch 900/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0968\n",
      "Epoch 901/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0971\n",
      "Epoch 902/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0968\n",
      "Epoch 903/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0969\n",
      "Epoch 904/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0975\n",
      "Epoch 905/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0994\n",
      "Epoch 906/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0973\n",
      "Epoch 907/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0974\n",
      "Epoch 908/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0971\n",
      "Epoch 909/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0968\n",
      "Epoch 910/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0999\n",
      "Epoch 911/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0978\n",
      "Epoch 912/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.1007\n",
      "Epoch 913/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0820 - val_loss: 0.0982\n",
      "Epoch 914/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0981\n",
      "Epoch 915/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0976\n",
      "Epoch 916/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0973\n",
      "Epoch 917/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 0.0971\n",
      "Epoch 918/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0988\n",
      "Epoch 919/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0972\n",
      "Epoch 920/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0991\n",
      "Epoch 921/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0807 - val_loss: 0.0968\n",
      "Epoch 922/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0983\n",
      "Epoch 923/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0973\n",
      "Epoch 924/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0977\n",
      "Epoch 925/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0974\n",
      "Epoch 926/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0986\n",
      "Epoch 927/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0971\n",
      "Epoch 928/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0979\n",
      "Epoch 929/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0971\n",
      "Epoch 930/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0968\n",
      "Epoch 931/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0977\n",
      "Epoch 932/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0974\n",
      "Epoch 933/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0978\n",
      "Epoch 934/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0988\n",
      "Epoch 935/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0970\n",
      "Epoch 936/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0967\n",
      "Epoch 937/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0968\n",
      "Epoch 938/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0971\n",
      "Epoch 939/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0971\n",
      "Epoch 940/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0972\n",
      "Epoch 941/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0976\n",
      "Epoch 942/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0968\n",
      "Epoch 943/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0979\n",
      "Epoch 944/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0976\n",
      "Epoch 945/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0970\n",
      "Epoch 946/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0976\n",
      "Epoch 947/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0979\n",
      "Epoch 948/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0981\n",
      "Epoch 949/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0972\n",
      "Epoch 950/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0979\n",
      "Epoch 951/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0972\n",
      "Epoch 952/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0968\n",
      "Epoch 953/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0968\n",
      "Epoch 954/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0974\n",
      "Epoch 955/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0984\n",
      "Epoch 956/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0972\n",
      "Epoch 957/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0972\n",
      "Epoch 958/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0970\n",
      "Epoch 959/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0987\n",
      "Epoch 960/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0969\n",
      "Epoch 961/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - val_loss: 0.0979\n",
      "Epoch 962/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.0968\n",
      "Epoch 963/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0968\n",
      "Epoch 964/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0968\n",
      "Epoch 965/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0973\n",
      "Epoch 966/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 0.0968\n",
      "Epoch 967/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.0967\n",
      "Epoch 968/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0967\n",
      "Epoch 969/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0970\n",
      "Epoch 970/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0968\n",
      "Epoch 971/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0973\n",
      "Epoch 972/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - val_loss: 0.0969\n",
      "Epoch 973/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 0.0986\n",
      "Epoch 974/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 0.0971\n",
      "Epoch 975/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0970\n",
      "Epoch 976/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0978\n",
      "Epoch 977/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0998\n",
      "Epoch 978/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0968\n",
      "Epoch 979/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0977\n",
      "Epoch 980/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0982\n",
      "Epoch 981/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0973\n",
      "Epoch 982/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0972\n",
      "Epoch 983/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0967\n",
      "Epoch 984/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.0973\n",
      "Epoch 985/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.0972\n",
      "Epoch 986/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - val_loss: 0.0979\n",
      "Epoch 987/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0795 - val_loss: 0.0969\n",
      "Epoch 988/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0982\n",
      "Epoch 989/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - val_loss: 0.0978\n",
      "Epoch 990/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0992\n",
      "Epoch 991/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 0.0968\n",
      "Epoch 992/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0968\n",
      "Epoch 993/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0971\n",
      "Epoch 994/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.0979\n",
      "Epoch 995/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0795 - val_loss: 0.0977\n",
      "Epoch 996/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 0.0968\n",
      "Epoch 997/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - val_loss: 0.0968\n",
      "Epoch 998/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.0978\n",
      "Epoch 999/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.1008\n",
      "Epoch 1000/1000\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 0.0971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8d407ef40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_validate_test, y_train, y_validate_test = train_test_split(X, Y,\n",
    "                                                                      test_size=0.2,\n",
    "                                                                      random_state=None)\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate_test,\n",
    "                                                          y_validate_test,\n",
    "                                                          test_size=0.5,\n",
    "                                                          random_state=None)\n",
    "\n",
    "model = buildModel(n_inputs=X_train.shape[1],\n",
    "                   n_hidden_layers=2,\n",
    "                   n_neurons=X_train.shape[1],\n",
    "                   learning_rate=0.00005)\n",
    "model.fit(X_train, y_train,\n",
    "          use_multiprocessing=True,\n",
    "          workers=8,\n",
    "          epochs=1000,\n",
    "          batch_size=10,\n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=100),\n",
    "                     keras.callbacks.TensorBoard(run_logdir)],\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.759940340481622"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(np.exp(predictions), np.exp(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09709370881319046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.501982815838593\n",
      "14.950655704768991\n",
      "15.49177704013593\n",
      "15.45095041270067\n",
      "18.123316049301433\n",
      "17.194806138171213\n",
      "16.199488572364196\n",
      "16.26995334226439\n",
      "16.278351953365203\n",
      "16.454787294576295\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(hist_descriptors[dist].getSampleHistogramData(bins)).values\n",
    "result_curv = evaluateModel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.924031004468125\n",
      "14.619562355493809\n",
      "13.824962192166833\n",
      "14.808838540983697\n",
      "13.305063343324255\n",
      "15.764872729559103\n",
      "12.610210684208504\n",
      "12.551503727937488\n",
      "14.898048785618569\n",
      "12.973796073298368\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(ma.hist_descriptors[dist].getSampleHistogram2dData(bins, True, False)).values\n",
    "result_curv_dist = evaluateModel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.19160693234869"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse, predicted, predicted_indices = result_curv #_dist\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.54184202376446\n",
      "16.127725071880576\n",
      "15.75150203657823\n",
      "13.662841579646246\n",
      "13.421435036370177\n",
      "13.379556889585176\n",
      "15.914756660517751\n",
      "14.053738121680704\n",
      "13.819117357133214\n",
      "14.337929455241492\n",
      "13.699536843415547\n",
      "13.473071169873727\n",
      "15.366817143178043\n",
      "13.633983592913218\n",
      "14.35541003778178\n",
      "13.752678521739531\n",
      "14.226764347784426\n",
      "14.572058674562205\n",
      "14.461617047826843\n",
      "14.146065103102847\n",
      "14.69009846141813\n",
      "13.233551389958826\n",
      "14.856309296047971\n",
      "13.990749999452015\n",
      "13.788257561976392\n",
      "14.561934334936199\n",
      "12.228210510017885\n",
      "13.482692751457321\n",
      "12.90060626673792\n",
      "12.753410506160058\n",
      "16.21523198342689\n",
      "13.753648956607195\n",
      "13.63987366994788\n",
      "14.398003279199662\n",
      "14.968089282623072\n",
      "11.846122867336431\n",
      "14.16296975961342\n",
      "14.19748493868112\n",
      "14.051633734110947\n",
      "12.819602778979036\n",
      "15.776878964232079\n",
      "15.132397924932729\n",
      "14.246343263468928\n",
      "14.228256454795565\n",
      "12.125368677799692\n",
      "13.713694914093868\n",
      "14.252187437324901\n",
      "13.54356921968613\n",
      "14.384394600917025\n",
      "12.326196866334197\n",
      "15.471391410142623\n",
      "12.213654411640755\n",
      "14.67490076842114\n",
      "14.87193463979599\n",
      "12.944523359382636\n",
      "14.713407491564858\n",
      "14.719121441142876\n",
      "15.560248599544636\n",
      "14.855325324402452\n",
      "13.345970342896575\n",
      "16.48431397280953\n",
      "17.17774375108396\n",
      "15.193341448846995\n",
      "13.701573021858144\n",
      "13.527117503425375\n",
      "12.837828347355597\n",
      "14.142627205050058\n",
      "14.716555842713436\n",
      "14.201605230306832\n",
      "14.319063556997543\n",
      "15.980995997841502\n",
      "12.260384160250444\n",
      "12.984473486088536\n",
      "16.214395976375357\n",
      "14.078086291163606\n",
      "14.364593103738603\n",
      "14.85719221977745\n",
      "14.45294123312232\n",
      "12.441568943258032\n",
      "14.253840456916151\n",
      "15.61532011385416\n",
      "14.004460598701847\n",
      "15.467404829441708\n",
      "16.209313907461155\n",
      "13.76220437738741\n",
      "15.009331893267357\n",
      "14.216173681204308\n",
      "12.125448791387063\n",
      "13.024013485300246\n",
      "13.110957926408547\n",
      "14.818974507825219\n",
      "15.590619454490941\n",
      "14.347334882641972\n",
      "13.38569072851533\n",
      "15.790347664057608\n",
      "13.784210966502453\n",
      "14.60104352548705\n",
      "14.538315048400296\n",
      "13.713031853509484\n",
      "14.517429880028478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.241585650206366"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=[]\n",
    "for i in range(10):\n",
    "    X = pd.DataFrame(ma.hist_descriptors[dist].getSampleHistogram2dData(bins, True, False)).values\n",
    "    result_curv_dist = evaluateModel(X, Y)\n",
    "    rmse, predicted, predicted_indices = result_curv_dist\n",
    "    r+=[rmse]\n",
    "np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "plt.scatter(np.exp(Y[predicted_indices.astype(int)]), np.exp(predicted))\n",
    "plt.xlabel(\"actual\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.plot([20, 100], [20, 100], color=\"black\", linewidth=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
